autointent.metrics.scoring
==========================

.. py:module:: autointent.metrics.scoring

.. autoapi-nested-parse::

   Scoring metrics for multiclass and multilabel classification tasks.



Attributes
----------

.. autoapisummary::

   autointent.metrics.scoring.logger


Classes
-------

.. autoapisummary::

   autointent.metrics.scoring.ScoringMetricFn


Functions
---------

.. autoapisummary::

   autointent.metrics.scoring.scoring_log_likelihood
   autointent.metrics.scoring.scoring_roc_auc
   autointent.metrics.scoring.scoring_accuracy
   autointent.metrics.scoring.scoring_f1
   autointent.metrics.scoring.scoring_precision
   autointent.metrics.scoring.scoring_recall
   autointent.metrics.scoring.scoring_hit_rate
   autointent.metrics.scoring.scoring_neg_coverage
   autointent.metrics.scoring.scoring_neg_ranking_loss
   autointent.metrics.scoring.scoring_map


Module Contents
---------------

.. py:data:: logger

.. py:class:: ScoringMetricFn

   Bases: :py:obj:`Protocol`


   Protocol for scoring metrics.


   .. py:method:: __call__(labels, scores)

      Calculate scoring metric.

      :param labels: ground truth labels for each utterance
          - multiclass case: list representing an array of shape `(n_samples,)` with integer values
          - multilabel case: list representing a matrix of shape `(n_samples, n_classes)` with integer values
      :param scores: for each utterance, this list contains scores for each of `n_classes` classes
      :return: Score of the scoring metric



.. py:function:: scoring_log_likelihood(labels, scores, eps = 1e-10)

   Supports multiclass and multilabel cases.

   Multiclass case:
   Mean negative cross-entropy for each utterance classification result:

   .. math::

       \frac{1}{\ell}\sum_{i=1}^{\ell}\log(s[y[i]])

   where ``s[y[i]]`` is the predicted score of the ``i``-th utterance having the ground truth label.

   Multilabel case:
   Mean negative binary cross-entropy:

   .. math::

       \frac{1}{\ell}\sum_{i=1}^\ell\sum_{c=1}^C\Big[y[i,c]\cdot\log(s[i,c])+(1-y[i,c])\cdot\log(1-s[i,c])\Big]

   where ``s[i,c]`` is the predicted score of the ``i``-th utterance having the ground truth label ``c``.

   :param labels: Ground truth labels for each utterance.
   :param scores: For each utterance, a list containing scores for each of `n_classes` classes.
   :param eps: A small value to avoid division by zero.
   :return: Score of the scoring metric.


.. py:function:: scoring_roc_auc(labels, scores)

   Supports multiclass and multilabel cases.

   Macro averaged roc-auc for utterance classification task, i.e.

   .. math::

       \frac{1}{C}\sum_{k=1}^C ROCAUC(scores[:, k], labels[:, k])

   where ``C`` is the number of classes

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_accuracy(labels, scores)

   Calculate accuracy for multiclass and multilabel classification.

   This function computes accuracy by using :func:`prediction_accuracy` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_f1(labels, scores)

   Calculate the F1 score for multiclass and multilabel classification.

   This function computes the F1 score by using :func:`prediction_f1` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: F1 score


.. py:function:: scoring_precision(labels, scores)

   Calculate precision for multiclass and multilabel classification.

   This function computes precision by using :func:`prediction_precision` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: Precision score


.. py:function:: scoring_recall(labels, scores)

   Calculate recall for multiclass and multilabel classification.

   This function computes recall by using :func:`prediction_recall` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: Recall score


.. py:function:: scoring_hit_rate(labels, scores)

   Calculate the hit rate for multilabel classification.

   The hit rate measures the fraction of cases where the top-ranked label is in the set
   of true labels for the instance.

   .. math::

       \text{Hit Rate} = \frac{1}{N} \sum_{i=1}^N \mathbb{1}(y_{\text{top},i} \in y_{\text{true},i})

   where:
   - :math:`N` is the total number of instances,
   - :math:`y_{\text{top},i}` is the top-ranked predicted label for instance :math:`i`,
   - :math:`y_{\text{true},i}` is the set of ground truth labels for instance :math:`i`.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: Hit rate score


.. py:function:: scoring_neg_coverage(labels, scores)

   Supports multilabel classification.

   Evaluates how far we need, on average, to go down the list of labels in order to cover
   all the proper labels of the instance.

   - The ideal value is 1
   - The worst value is 0

   The result is equivalent to executing the following code:

   >>> def compute_rank_metric():
   ...     import numpy as np
   ...     scores = np.array([[1, 2, 3]])
   ...     labels = np.array([1, 0, 0])
   ...     n_classes = scores.shape[1]
   ...     from scipy.stats import rankdata
   ...     int_ranks = rankdata(scores, axis=1)
   ...     filtered_ranks = int_ranks * labels
   ...     max_ranks = np.max(filtered_ranks, axis=1)
   ...     float_ranks = (max_ranks - 1) / (n_classes - 1)
   ...     return float(1 - np.mean(float_ranks))
   >>> print(f"{compute_rank_metric():.1f}")
   1.0

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_neg_ranking_loss(labels, scores)

   supports multilabel.

   Compute the average number of label pairs that are incorrectly ordered given y_score
   weighted by the size of the label set and the number of labels not in the label set.

   the ideal value is 0

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_map(labels, scores)

   Calculate the mean average precision (MAP) score for multilabel classification.

   The MAP score measures the precision at different levels of ranking,
   averaged across all queries. The ideal value is 1, indicating perfect ranking, while the worst value is 0.

   This function utilizes :func:`sklearn.metrics.label_ranking_average_precision_score` for computation.

   :param labels: ground truth labels for each sample
   :param scores: for each sample, this list contains scores for each of `n_classes` classes
   :return: mean average precision score


