autointent.metrics
==================

.. py:module:: autointent.metrics


Submodules
----------

.. toctree::
   :maxdepth: 1

   /autoapi/autointent/metrics/prediction/index
   /autoapi/autointent/metrics/regexp/index
   /autoapi/autointent/metrics/retrieval/index
   /autoapi/autointent/metrics/scoring/index


Attributes
----------

.. autoapisummary::

   autointent.metrics.METRIC_FN


Classes
-------

.. autoapisummary::

   autointent.metrics.PredictionMetricFn
   autointent.metrics.RegexpMetricFn
   autointent.metrics.RetrievalMetricFn
   autointent.metrics.ScoringMetricFn


Functions
---------

.. autoapisummary::

   autointent.metrics.prediction_accuracy
   autointent.metrics.prediction_f1
   autointent.metrics.prediction_precision
   autointent.metrics.prediction_recall
   autointent.metrics.prediction_roc_auc
   autointent.metrics.regexp_partial_accuracy
   autointent.metrics.regexp_partial_precision
   autointent.metrics.retrieval_hit_rate
   autointent.metrics.retrieval_hit_rate_intersecting
   autointent.metrics.retrieval_hit_rate_macro
   autointent.metrics.retrieval_map
   autointent.metrics.retrieval_map_intersecting
   autointent.metrics.retrieval_map_macro
   autointent.metrics.retrieval_mrr
   autointent.metrics.retrieval_mrr_intersecting
   autointent.metrics.retrieval_mrr_macro
   autointent.metrics.retrieval_ndcg
   autointent.metrics.retrieval_ndcg_intersecting
   autointent.metrics.retrieval_ndcg_macro
   autointent.metrics.retrieval_precision
   autointent.metrics.retrieval_precision_intersecting
   autointent.metrics.retrieval_precision_macro
   autointent.metrics.scoring_accuracy
   autointent.metrics.scoring_f1
   autointent.metrics.scoring_hit_rate
   autointent.metrics.scoring_log_likelihood
   autointent.metrics.scoring_map
   autointent.metrics.scoring_neg_coverage
   autointent.metrics.scoring_neg_ranking_loss
   autointent.metrics.scoring_precision
   autointent.metrics.scoring_recall
   autointent.metrics.scoring_roc_auc


Package Contents
----------------

.. py:class:: PredictionMetricFn

   Bases: :py:obj:`Protocol`


   Protocol for prediction metrics.


   .. py:method:: __call__(y_true, y_pred)

      Calculate prediction metric.

      :param y_true: True values of labels
          - multiclass case: list representing an array shape `(n_samples,)` of integer class labels
          - multilabel case: list representing a matrix of shape `(n_samples, n_classes)` with binary values
      :param y_pred: Predicted values of labels. Same shape as `y_true`
      :return: Score of the prediction metric



.. py:function:: prediction_accuracy(y_true, y_pred)

   Calculate prediction accuracy. Supports both multiclass and multilabel.

   The prediction accuracy is calculated as:

   .. math::

       \text{Accuracy} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{true},i} = y_{\text{pred},i})}{N}

   where:
   - :math:`N` is the total number of samples,
   - :math:`y_{\text{true},i}` is the true label for the :math:`i`-th sample,
   - :math:`y_{\text{pred},i}` is the predicted label for the :math:`i`-th sample,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the prediction accuracy


.. py:function:: prediction_f1(y_true, y_pred)

   Calculate prediction f1 score. Supports both multiclass and multilabel.

   This function internally uses :func:`sklearn.metrics.f1_score` with `average=macro`. Refer to the
   `scikit-learn documentation <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html>`__
   for more details.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the prediction accuracy


.. py:function:: prediction_precision(y_true, y_pred)

   Calculate prediction precision. Supports both multiclass and multilabel.

   This function internally uses :func:`sklearn.metrics.precision_score` with `average=macro`. Refer to the
   `scikit-learn documentation <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html>`__
   for more details.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the prediction precision


.. py:function:: prediction_recall(y_true, y_pred)

   Calculate prediction recall. Supports both multiclass and multilabel.

   This function internally uses :func:`sklearn.metrics.recall_score` with `average=macro`. Refer to the
   `scikit-learn documentation <https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html>`__
   for more details.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the prediction recall


.. py:function:: prediction_roc_auc(y_true, y_pred)

   Calculate ROC AUC for multiclass and multilabel classification.

   The ROC AUC measures the ability of a model to distinguish between classes.
   It is calculated as the area under the curve of the true positive rate (TPR)
   against the false positive rate (FPR) at various threshold settings.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the prediction ROC AUC


.. py:class:: RegexpMetricFn

   Bases: :py:obj:`Protocol`


   Protocol for regexp metrics.


   .. py:method:: __call__(y_true, y_pred)

      Calculate regexp metric.

      :param y_true: True values of labels
      :param y_pred: Predicted values of labels
      :return: Score of the regexp metric



.. py:function:: regexp_partial_accuracy(y_true, y_pred)

   Calculate regexp partial accuracy.

   The regexp partial accuracy is calculated as:

   .. math::

       \text{Partial Accuracy} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{true},i} \in y_{\text{pred},i})}{N}

   where:
   - :math:`N` is the total number of samples,
   - :math:`y_{\text{true},i}` is the true label for the :math:`i`-th sample,
   - :math:`y_{\text{pred},i}` is the predicted label for the :math:`i`-th sample,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the regexp metric


.. py:function:: regexp_partial_precision(y_true, y_pred)

   Calculate regexp partial precision.

   The regexp partial precision is calculated as:

   .. math::

       \text{Partial Precision} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{true},i}
       \in y_{\text{pred},i})}{\sum_{i=1}^N \mathbb{1}(|y_{\text{pred},i}| > 0)}

   where:
   - :math:`N` is the total number of samples,
   - :math:`y_{\text{true},i}` is the true label for the :math:`i`-th sample,
   - :math:`y_{\text{pred},i}` is the predicted label for the :math:`i`-th sample,
   - :math:`|y_{\text{pred},i}|` is the number of predicted labels for the :math:`i`-th sample,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param y_true: True values of labels
   :param y_pred: Predicted values of labels
   :return: Score of the regexp metric


.. py:class:: RetrievalMetricFn

   Bases: :py:obj:`Protocol`


   Protocol for retrieval metrics.


   .. py:method:: __call__(query_labels, candidates_labels, k = None)

      Calculate retrieval metric.

      - multiclass case: labels are integer
      - multilabel case: labels are binary


      :param query_labels: For each query, this list contains its class labels
      :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
       (from most to least relevant)
      :param k: Number of top items to consider for each query
      :return: Score of the retrieval metric



.. py:function:: retrieval_hit_rate(query_labels, candidates_labels, k = None)

   Calculate the hit rate at position k.

   The hit rate is calculated as:

   .. math::

       \text{Hit Rate} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{query},i} \in y_{\text{candidates},i}^{(1:k)})}{N}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_hit_rate_intersecting(query_labels, candidates_labels, k = None)

   Calculate the hit rate at position k for the intersecting labels.

   The intersecting hit rate is calculated as:

   .. math::

       \text{Hit Rate}_{\text{intersecting}} = \frac{\sum_{i=1}^N \mathbb{1} \left( \sum_{j=1}^k
       \left( y_{\text{query},i} \cdot y_{\text{candidates},i,j} \right) > 0 \right)}{N}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the one-hot encoded label vector for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i,j}` is the one-hot encoded label vector of the :math:`j`-th
   candidate for the :math:`i`-th query,
   - :math:`k` is the number of top candidates considered,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_hit_rate_macro(query_labels, candidates_labels, k = None)

   Calculate the hit rate at position k for the intersecting labels.

   This function internally uses :func:`retrieval_hit_rate` to calculate the hit rate at position :math:`k`
   for each query and applies :func:`macrofy` to perform macro-averaging across multiple queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_map(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k.

   The Mean Average Precision (MAP) is computed as the average of the average precision
   (AP) scores for all queries. The average precision for a single query is calculated using
   the :func:`average_precision` function, which computes the precision at each rank
   position considering the top-k retrieved items.

   MAP is given by:

   .. math::

       \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}(q, c, k)

   where:
   - :math:`Q` is the total number of queries,
   - :math:`\text{AP}(q, c, k)` is the average precision for the :math:`q`-th query,
   calculated considering the true labels for that query :math:`q`, the ranked candidate
   labels :math:`c`, and the number `k` which determines the number of top items to consider.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_map_intersecting(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k for the intersecting labels.

   The Mean Average Precision (MAP) for intersecting labels is computed as
   the average of the average precision (AP) scores for all queries. The average
   precision for a single query is calculated using the :func:`average_precision_intersecting`
   function, which considers the intersecting true and predicted labels for the
   top-k retrieved items.

   MAP is given by:

   .. math::

       \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}_{\text{intersecting}}(q, c, k)

   where:
   - :math:`Q` is the total number of queries,
   - :math:`\text{AP}_{\text{intersecting}}(q, c, k)` is the average precision for the
   :math:`q`-th query, calculated using the intersecting true labels (`q`),
   predicted labels (`c`), and the number of top items (`k`) to consider.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_map_macro(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k for the intersecting labels.

   This function internally uses :func:`retrieval_map` to calculate the MAP for each query and then
   applies :func:`macrofy` to perform macro-averaging across multiple queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_mrr(query_labels, candidates_labels, k = None)

   Calculate the Mean Reciprocal Rank (MRR) at position k.

   MRR is calculated as:

   .. math::

       \text{MRR@k} = \frac{1}{N} \sum_{i=1}^N \frac{1}{\text{rank}_i}

   where:
   - :math:`\text{rank}_i` is the rank position of the first relevant item in the top-k results for query :math:`i`,
   - :math:`N` is the total number of queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_mrr_intersecting(query_labels, candidates_labels, k = None)

   Calculate the Mean Reciprocal Rank (MRR) at position k for the intersecting labels.

   MRR is calculated as:

   .. math::

       \text{MRR@k}_{\text{intersecting}} = \frac{1}{N} \sum_{i=1}^N \frac{1}{\text{rank}_i}

   where:
   - :math:`\text{rank}_i` is the rank position of the first relevant (intersecting) item in the top-k
   results for query :math:`i`,
   - :math:`N` is the total number of queries.

   Intersecting relevance is determined by checking whether the query label intersects with the candidate labels.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_mrr_macro(query_labels, candidates_labels, k = None)

   Calculate the Mean Reciprocal Rank (MRR) at position k for the intersecting labels.

   This function calculates MRR using :func:`retrieval_mrr` and applies it to each
   query using :func:`macrofy` to compute the macro-averaged score.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_ndcg(query_labels, candidates_labels, k = None)

   Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k.

   NDCG at position :math:`k` is calculated as:

   .. math::

       \text{NDCG@k} = \frac{\text{DCG@k}}{\text{IDCG@k}}

   where:
   - :math:`\text{DCG@k}` is the Discounted Cumulative Gain at position :math:`k`,
   - :math:`\text{IDCG@k}` is the Ideal Discounted Cumulative Gain at position :math:`k`.

   The NDCG value is normalized such that it is between 0 and 1, where 1 indicates the ideal ranking.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_ndcg_intersecting(query_labels, candidates_labels, k = None)

   Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k for the intersecting labels.

   NDCG at position :math:`k` for intersecting labels is calculated as:

   .. math::

       \text{NDCG@k}_{\text{intersecting}} = \frac{\text{DCG@k}_{\text{intersecting}}}
       {\text{IDCG@k}_{\text{intersecting}}}

   where:

   - :math:`\text{DCG@k}_{\text{intersecting}}` is the Discounted Cumulative Gain for the intersecting labels at position :math:`k`,
   - :math:`\text{IDCG@k}_{\text{intersecting}}` is the Ideal Discounted Cumulative Gain for the intersecting labels at position :math:`k`.

   Intersecting relevance is determined by checking whether the query labels overlap with
   the candidate labels.
   NDCG values are normalized between 0 and 1, where 1 indicates the ideal ranking.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_ndcg_macro(query_labels, candidates_labels, k = None)

   Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k for the intersecting labels.

   This function calculates NDCG using :func:`retrieval_ndcg` and applies it to each
   query using :func:`macrofy` to compute the macro-averaged score.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_precision(query_labels, candidates_labels, k = None)

   Calculate the precision at position k.

   Precision at position :math:`k` is calculated as:

   .. math::

       \text{Precision@k} = \frac{1}{N} \sum_{i=1}^N \frac{|y_{\text{query},i} \cap
       y_{\text{candidates},i}^{(1:k)}|}{k}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_precision_intersecting(query_labels, candidates_labels, k = None)

   Calculate the precision at position k for the intersecting labels.

   Precision at position :math:`k` for intersecting labels is calculated as:

   .. math::

       \text{Precision@k}_{\text{intersecting}} = \frac{1}{N} \sum_{i=1}^N
       \frac{\sum_{j=1}^k \mathbb{1} \left( y_{\text{query},i} \cdot y_{\text{candidates},i,j} > 0 \right)}{k}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the one-hot encoded label vector for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i,j}` is the one-hot encoded label vector of the :math:`j`-th
   candidate for the :math:`i`-th query,
   - :math:`k` is the number of top candidates considered,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the
   condition is true and 0 otherwise.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_precision_macro(query_labels, candidates_labels, k = None)

   Calculate the precision at position k for the intersecting labels.

   This function internally uses :func:`retrieval_precision` to calculate the precision at position :math:`k`
   for each query and applies :func:`macrofy` to perform macro-averaging across multiple queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:class:: ScoringMetricFn

   Bases: :py:obj:`Protocol`


   Protocol for scoring metrics.


   .. py:method:: __call__(labels, scores)

      Calculate scoring metric.

      :param labels: ground truth labels for each utterance
          - multiclass case: list representing an array of shape `(n_samples,)` with integer values
          - multilabel case: list representing a matrix of shape `(n_samples, n_classes)` with integer values
      :param scores: for each utterance, this list contains scores for each of `n_classes` classes
      :return: Score of the scoring metric



.. py:function:: scoring_accuracy(labels, scores)

   Calculate accuracy for multiclass and multilabel classification.

   This function computes accuracy by using :func:`prediction_accuracy` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_f1(labels, scores)

   Calculate the F1 score for multiclass and multilabel classification.

   This function computes the F1 score by using :func:`prediction_f1` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: F1 score


.. py:function:: scoring_hit_rate(labels, scores)

   Calculate the hit rate for multilabel classification.

   The hit rate measures the fraction of cases where the top-ranked label is in the set
   of true labels for the instance.

   .. math::

       \text{Hit Rate} = \frac{1}{N} \sum_{i=1}^N \mathbb{1}(y_{\text{top},i} \in y_{\text{true},i})

   where:
   - :math:`N` is the total number of instances,
   - :math:`y_{\text{top},i}` is the top-ranked predicted label for instance :math:`i`,
   - :math:`y_{\text{true},i}` is the set of ground truth labels for instance :math:`i`.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: Hit rate score


.. py:function:: scoring_log_likelihood(labels, scores, eps = 1e-10)

   Supports multiclass and multilabel cases.

   Multiclass case:
   Mean negative cross-entropy for each utterance classification result:

   .. math::

       \frac{1}{\ell}\sum_{i=1}^{\ell}\log(s[y[i]])

   where ``s[y[i]]`` is the predicted score of the ``i``-th utterance having the ground truth label.

   Multilabel case:
   Mean negative binary cross-entropy:

   .. math::

       \frac{1}{\ell}\sum_{i=1}^\ell\sum_{c=1}^C\Big[y[i,c]\cdot\log(s[i,c])+(1-y[i,c])\cdot\log(1-s[i,c])\Big]

   where ``s[i,c]`` is the predicted score of the ``i``-th utterance having the ground truth label ``c``.

   :param labels: Ground truth labels for each utterance.
   :param scores: For each utterance, a list containing scores for each of `n_classes` classes.
   :param eps: A small value to avoid division by zero.
   :return: Score of the scoring metric.


.. py:function:: scoring_map(labels, scores)

   Calculate the mean average precision (MAP) score for multilabel classification.

   The MAP score measures the precision at different levels of ranking,
   averaged across all queries. The ideal value is 1, indicating perfect ranking, while the worst value is 0.

   This function utilizes :func:`sklearn.metrics.label_ranking_average_precision_score` for computation.

   :param labels: ground truth labels for each sample
   :param scores: for each sample, this list contains scores for each of `n_classes` classes
   :return: mean average precision score


.. py:function:: scoring_neg_coverage(labels, scores)

   Supports multilabel classification.

   Evaluates how far we need, on average, to go down the list of labels in order to cover
   all the proper labels of the instance.

   - The ideal value is 1
   - The worst value is 0

   The result is equivalent to executing the following code:

   >>> def compute_rank_metric():
   ...     import numpy as np
   ...     scores = np.array([[1, 2, 3]])
   ...     labels = np.array([1, 0, 0])
   ...     n_classes = scores.shape[1]
   ...     from scipy.stats import rankdata
   ...     int_ranks = rankdata(scores, axis=1)
   ...     filtered_ranks = int_ranks * labels
   ...     max_ranks = np.max(filtered_ranks, axis=1)
   ...     float_ranks = (max_ranks - 1) / (n_classes - 1)
   ...     return float(1 - np.mean(float_ranks))
   >>> print(f"{compute_rank_metric():.1f}")
   1.0

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_neg_ranking_loss(labels, scores)

   supports multilabel.

   Compute the average number of label pairs that are incorrectly ordered given y_score
   weighted by the size of the label set and the number of labels not in the label set.

   the ideal value is 0

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:function:: scoring_precision(labels, scores)

   Calculate precision for multiclass and multilabel classification.

   This function computes precision by using :func:`prediction_precision` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: Precision score


.. py:function:: scoring_recall(labels, scores)

   Calculate recall for multiclass and multilabel classification.

   This function computes recall by using :func:`prediction_recall` to evaluate predictions and
   :func:`calculate_prediction_metric` to handle the computation.

   :param labels: Ground truth labels for each sample
   :param scores: For each sample, this list contains scores for each of `n_classes` classes
   :return: Recall score


.. py:function:: scoring_roc_auc(labels, scores)

   Supports multiclass and multilabel cases.

   Macro averaged roc-auc for utterance classification task, i.e.

   .. math::

       \frac{1}{C}\sum_{k=1}^C ROCAUC(scores[:, k], labels[:, k])

   where ``C`` is the number of classes

   :param labels: ground truth labels for each utterance
   :param scores: for each utterance, this list contains scores for each of `n_classes` classes
   :return: Score of the scoring metric


.. py:data:: METRIC_FN

