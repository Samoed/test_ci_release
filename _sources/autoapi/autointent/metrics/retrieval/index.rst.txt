autointent.metrics.retrieval
============================

.. py:module:: autointent.metrics.retrieval

.. autoapi-nested-parse::

   Retrieval metrics.



Classes
-------

.. autoapisummary::

   autointent.metrics.retrieval.RetrievalMetricFn


Functions
---------

.. autoapisummary::

   autointent.metrics.retrieval.retrieval_map
   autointent.metrics.retrieval.retrieval_map_intersecting
   autointent.metrics.retrieval.retrieval_map_macro
   autointent.metrics.retrieval.retrieval_hit_rate
   autointent.metrics.retrieval.retrieval_hit_rate_intersecting
   autointent.metrics.retrieval.retrieval_hit_rate_macro
   autointent.metrics.retrieval.retrieval_precision
   autointent.metrics.retrieval.retrieval_precision_intersecting
   autointent.metrics.retrieval.retrieval_precision_macro
   autointent.metrics.retrieval.retrieval_ndcg
   autointent.metrics.retrieval.retrieval_ndcg_intersecting
   autointent.metrics.retrieval.retrieval_ndcg_macro
   autointent.metrics.retrieval.retrieval_mrr
   autointent.metrics.retrieval.retrieval_mrr_intersecting
   autointent.metrics.retrieval.retrieval_mrr_macro


Module Contents
---------------

.. py:class:: RetrievalMetricFn

   Bases: :py:obj:`Protocol`


   Protocol for retrieval metrics.


   .. py:method:: __call__(query_labels, candidates_labels, k = None)

      Calculate retrieval metric.

      - multiclass case: labels are integer
      - multilabel case: labels are binary


      :param query_labels: For each query, this list contains its class labels
      :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
       (from most to least relevant)
      :param k: Number of top items to consider for each query
      :return: Score of the retrieval metric



.. py:function:: retrieval_map(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k.

   The Mean Average Precision (MAP) is computed as the average of the average precision
   (AP) scores for all queries. The average precision for a single query is calculated using
   the :func:`average_precision` function, which computes the precision at each rank
   position considering the top-k retrieved items.

   MAP is given by:

   .. math::

       \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}(q, c, k)

   where:
   - :math:`Q` is the total number of queries,
   - :math:`\text{AP}(q, c, k)` is the average precision for the :math:`q`-th query,
   calculated considering the true labels for that query :math:`q`, the ranked candidate
   labels :math:`c`, and the number `k` which determines the number of top items to consider.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_map_intersecting(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k for the intersecting labels.

   The Mean Average Precision (MAP) for intersecting labels is computed as
   the average of the average precision (AP) scores for all queries. The average
   precision for a single query is calculated using the :func:`average_precision_intersecting`
   function, which considers the intersecting true and predicted labels for the
   top-k retrieved items.

   MAP is given by:

   .. math::

       \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}_{\text{intersecting}}(q, c, k)

   where:
   - :math:`Q` is the total number of queries,
   - :math:`\text{AP}_{\text{intersecting}}(q, c, k)` is the average precision for the
   :math:`q`-th query, calculated using the intersecting true labels (`q`),
   predicted labels (`c`), and the number of top items (`k`) to consider.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_map_macro(query_labels, candidates_labels, k = None)

   Calculate the mean average precision at position k for the intersecting labels.

   This function internally uses :func:`retrieval_map` to calculate the MAP for each query and then
   applies :func:`macrofy` to perform macro-averaging across multiple queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_hit_rate(query_labels, candidates_labels, k = None)

   Calculate the hit rate at position k.

   The hit rate is calculated as:

   .. math::

       \text{Hit Rate} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{query},i} \in y_{\text{candidates},i}^{(1:k)})}{N}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_hit_rate_intersecting(query_labels, candidates_labels, k = None)

   Calculate the hit rate at position k for the intersecting labels.

   The intersecting hit rate is calculated as:

   .. math::

       \text{Hit Rate}_{\text{intersecting}} = \frac{\sum_{i=1}^N \mathbb{1} \left( \sum_{j=1}^k
       \left( y_{\text{query},i} \cdot y_{\text{candidates},i,j} \right) > 0 \right)}{N}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the one-hot encoded label vector for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i,j}` is the one-hot encoded label vector of the :math:`j`-th
   candidate for the :math:`i`-th query,
   - :math:`k` is the number of top candidates considered,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition
   is true and 0 otherwise.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_hit_rate_macro(query_labels, candidates_labels, k = None)

   Calculate the hit rate at position k for the intersecting labels.

   This function internally uses :func:`retrieval_hit_rate` to calculate the hit rate at position :math:`k`
   for each query and applies :func:`macrofy` to perform macro-averaging across multiple queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_precision(query_labels, candidates_labels, k = None)

   Calculate the precision at position k.

   Precision at position :math:`k` is calculated as:

   .. math::

       \text{Precision@k} = \frac{1}{N} \sum_{i=1}^N \frac{|y_{\text{query},i} \cap
       y_{\text{candidates},i}^{(1:k)}|}{k}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_precision_intersecting(query_labels, candidates_labels, k = None)

   Calculate the precision at position k for the intersecting labels.

   Precision at position :math:`k` for intersecting labels is calculated as:

   .. math::

       \text{Precision@k}_{\text{intersecting}} = \frac{1}{N} \sum_{i=1}^N
       \frac{\sum_{j=1}^k \mathbb{1} \left( y_{\text{query},i} \cdot y_{\text{candidates},i,j} > 0 \right)}{k}

   where:
   - :math:`N` is the total number of queries,
   - :math:`y_{\text{query},i}` is the one-hot encoded label vector for the :math:`i`-th query,
   - :math:`y_{\text{candidates},i,j}` is the one-hot encoded label vector of the :math:`j`-th
   candidate for the :math:`i`-th query,
   - :math:`k` is the number of top candidates considered,
   - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the
   condition is true and 0 otherwise.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_precision_macro(query_labels, candidates_labels, k = None)

   Calculate the precision at position k for the intersecting labels.

   This function internally uses :func:`retrieval_precision` to calculate the precision at position :math:`k`
   for each query and applies :func:`macrofy` to perform macro-averaging across multiple queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_ndcg(query_labels, candidates_labels, k = None)

   Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k.

   NDCG at position :math:`k` is calculated as:

   .. math::

       \text{NDCG@k} = \frac{\text{DCG@k}}{\text{IDCG@k}}

   where:
   - :math:`\text{DCG@k}` is the Discounted Cumulative Gain at position :math:`k`,
   - :math:`\text{IDCG@k}` is the Ideal Discounted Cumulative Gain at position :math:`k`.

   The NDCG value is normalized such that it is between 0 and 1, where 1 indicates the ideal ranking.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model
    (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_ndcg_intersecting(query_labels, candidates_labels, k = None)

   Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k for the intersecting labels.

   NDCG at position :math:`k` for intersecting labels is calculated as:

   .. math::

       \text{NDCG@k}_{\text{intersecting}} = \frac{\text{DCG@k}_{\text{intersecting}}}
       {\text{IDCG@k}_{\text{intersecting}}}

   where:

   - :math:`\text{DCG@k}_{\text{intersecting}}` is the Discounted Cumulative Gain for the intersecting labels at position :math:`k`,
   - :math:`\text{IDCG@k}_{\text{intersecting}}` is the Ideal Discounted Cumulative Gain for the intersecting labels at position :math:`k`.

   Intersecting relevance is determined by checking whether the query labels overlap with
   the candidate labels.
   NDCG values are normalized between 0 and 1, where 1 indicates the ideal ranking.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_ndcg_macro(query_labels, candidates_labels, k = None)

   Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k for the intersecting labels.

   This function calculates NDCG using :func:`retrieval_ndcg` and applies it to each
   query using :func:`macrofy` to compute the macro-averaged score.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_mrr(query_labels, candidates_labels, k = None)

   Calculate the Mean Reciprocal Rank (MRR) at position k.

   MRR is calculated as:

   .. math::

       \text{MRR@k} = \frac{1}{N} \sum_{i=1}^N \frac{1}{\text{rank}_i}

   where:
   - :math:`\text{rank}_i` is the rank position of the first relevant item in the top-k results for query :math:`i`,
   - :math:`N` is the total number of queries.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_mrr_intersecting(query_labels, candidates_labels, k = None)

   Calculate the Mean Reciprocal Rank (MRR) at position k for the intersecting labels.

   MRR is calculated as:

   .. math::

       \text{MRR@k}_{\text{intersecting}} = \frac{1}{N} \sum_{i=1}^N \frac{1}{\text{rank}_i}

   where:
   - :math:`\text{rank}_i` is the rank position of the first relevant (intersecting) item in the top-k
   results for query :math:`i`,
   - :math:`N` is the total number of queries.

   Intersecting relevance is determined by checking whether the query label intersects with the candidate labels.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


.. py:function:: retrieval_mrr_macro(query_labels, candidates_labels, k = None)

   Calculate the Mean Reciprocal Rank (MRR) at position k for the intersecting labels.

   This function calculates MRR using :func:`retrieval_mrr` and applies it to each
   query using :func:`macrofy` to compute the macro-averaged score.

   :param query_labels: For each query, this list contains its class labels
   :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)
   :param k: Number of top items to consider for each query
   :return: Score of the retrieval metric


