<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>autointent.metrics.retrieval &#8212; AutoIntent 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=686e5160" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <script src="../../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for autointent.metrics.retrieval</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Retrieval metrics.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Callable</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Protocol</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">numpy.typing</span> <span class="k">as</span> <span class="nn">npt</span>

<span class="kn">from</span> <span class="nn">._converter</span> <span class="kn">import</span> <span class="n">transform</span>
<span class="kn">from</span> <span class="nn">._custom_types</span> <span class="kn">import</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">LABELS_VALUE_TYPE</span>


<div class="viewcode-block" id="RetrievalMetricFn">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.RetrievalMetricFn">[docs]</a>
<span class="k">class</span> <span class="nc">RetrievalMetricFn</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Protocol for retrieval metrics.&quot;&quot;&quot;</span>

<div class="viewcode-block" id="RetrievalMetricFn.__call__">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.RetrievalMetricFn.__call__">[docs]</a>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
        <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
        <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculate retrieval metric.</span>

<span class="sd">        - multiclass case: labels are integer</span>
<span class="sd">        - multilabel case: labels are binary</span>


<span class="sd">        :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">        :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">         (from most to least relevant)</span>
<span class="sd">        :param k: Number of top items to consider for each query</span>
<span class="sd">        :return: Score of the retrieval metric</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="o">...</span></div>
</div>



<span class="k">def</span> <span class="nf">_macrofy</span><span class="p">(</span>
    <span class="n">metric_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extend single-label `metric_fn` to a multi-label case via macro averaging.</span>

<span class="sd">    The macro-average score is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{MacroAvg} = \frac{1}{C} \sum_{i=1}^{C} \text{metric}(y_{\text{true},i}, y_{\text{pred},i}, k)</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`C` is the number of classes,</span>
<span class="sd">    - :math:`y_{\text{true},i}` is the true binary indicator for the :math:`i`-th class label,</span>
<span class="sd">    - :math:`y_{\text{pred},i}` is the predicted binary indicator for the :math:`i`-th class label,</span>
<span class="sd">    - :math:`k` is the number of top predictions to consider for each query,</span>
<span class="sd">    - :math:`\text{metric}(y_{\text{true},i}, y_{\text{pred},i}, k)`</span>
<span class="sd">    is the metric function applied to the top-k predictions for each class.</span>

<span class="sd">    :param metric_fn: Metric function</span>
<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_labels_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>

    <span class="n">n_classes</span> <span class="o">=</span> <span class="n">query_labels_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">classwise_values</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
        <span class="n">binarized_query_labels</span> <span class="o">=</span> <span class="n">query_labels_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">binarized_candidates_labels</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">classwise_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_fn</span><span class="p">(</span><span class="n">binarized_query_labels</span><span class="p">,</span> <span class="n">binarized_candidates_labels</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">classwise_values</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value]</span>


<span class="k">def</span> <span class="nf">_average_precision</span><span class="p">(</span><span class="n">query_label</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">candidate_labels</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">],</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the average precision at position k.</span>

<span class="sd">    The average precision is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{AP} = \frac{1}{\text{num_relevant}} \sum_{i=1}^{k} \mathbb{1}(y_{\text{true},i} = 1)</span>
<span class="sd">        \cdot \frac{\text{num_relevant}}{i+1}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`k` is the number of top items to consider for each query,</span>
<span class="sd">    - :math:`\text{num_relevant}` is the number of relevant items in the top-k ranking,</span>
<span class="sd">    - :math:`y_{\text{true},i}` is the true label (query label) for the :math:`i`-th ranked item,</span>
<span class="sd">    - :math:`\mathbb{1}(y_{\text{true},i} = 1)` is the indicator function that equals 1 if the</span>
<span class="sd">    :math:`i`-th item is relevant,</span>
<span class="sd">    - :math:`\frac{\text{num_relevant}}{i+1}` is the precision at rank :math:`i`.</span>

<span class="sd">    :param query_label: For each query, this list contains its class labels</span>
<span class="sd">    :param candidate_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">num_relevant</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sum_precision</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_labels</span><span class="p">[:</span><span class="n">k</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="n">query_label</span><span class="p">:</span>
            <span class="n">num_relevant</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">sum_precision</span> <span class="o">+=</span> <span class="n">num_relevant</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sum_precision</span> <span class="o">/</span> <span class="n">num_relevant</span> <span class="k">if</span> <span class="n">num_relevant</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>


<div class="viewcode-block" id="retrieval_map">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_map">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_map</span><span class="p">(</span><span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the mean average precision at position k.</span>

<span class="sd">    The Mean Average Precision (MAP) is computed as the average of the average precision</span>
<span class="sd">    (AP) scores for all queries. The average precision for a single query is calculated using</span>
<span class="sd">    the :func:`average_precision` function, which computes the precision at each rank</span>
<span class="sd">    position considering the top-k retrieved items.</span>

<span class="sd">    MAP is given by:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}(q, c, k)</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`Q` is the total number of queries,</span>
<span class="sd">    - :math:`\text{AP}(q, c, k)` is the average precision for the :math:`q`-th query,</span>
<span class="sd">    calculated considering the true labels for that query :math:`q`, the ranked candidate</span>
<span class="sd">    labels :math:`c`, and the number `k` which determines the number of top items to consider.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ap_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">_average_precision</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ap_list</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ap_list</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_average_precision_intersecting</span><span class="p">(</span>
    <span class="n">query_label</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidate_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the average precision at position k for the intersecting labels.</span>

<span class="sd">    The average precision for intersecting labels is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{AP} = \frac{1}{\text{num_relevant}} \sum_{i=1}^{k} \mathbb{1}\left(\sum_{j=1}^{C}</span>
<span class="sd">        y_{\text{true},j}(q) \cdot y_{\text{pred},j}(i) &gt; 0 \right) \cdot \frac{\text{num_relevant}}{i+1}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`k` is the number of top items to consider for each query,</span>
<span class="sd">    - :math:`\text{num_relevant}` is the number of relevant items in the top-k ranking,</span>
<span class="sd">    - :math:`y_{\text{true},j}(q)` is the true binary label for the :math:`j`-th</span>
<span class="sd">    class of the query :math:`q`,</span>
<span class="sd">    - :math:`y_{\text{pred},j}(i)` is the predicted binary label for the :math:`j`-th class</span>
<span class="sd">    of the :math:`i`-th ranked item,</span>
<span class="sd">    - :math:`\mathbb{1}(\cdot)` is the indicator function that equals 1 if the sum of the</span>
<span class="sd">    element-wise product of true and predicted labels is greater than 0, and 0 otherwise,</span>
<span class="sd">    - :math:`\frac{\text{num_relevant}}{i+1}` is the precision at rank :math:`i`.</span>

<span class="sd">    :param query_label: For each query, this list contains its class labels</span>
<span class="sd">    :param candidate_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidate_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span><span class="p">)</span>

    <span class="n">num_relevant</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">sum_precision</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_labels_</span><span class="p">[:</span><span class="n">k</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">query_label_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">num_relevant</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">sum_precision</span> <span class="o">+=</span> <span class="n">num_relevant</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sum_precision</span> <span class="o">/</span> <span class="n">num_relevant</span> <span class="k">if</span> <span class="n">num_relevant</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mf">0.0</span>


<div class="viewcode-block" id="retrieval_map_intersecting">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_map_intersecting">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_map_intersecting</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the mean average precision at position k for the intersecting labels.</span>

<span class="sd">    The Mean Average Precision (MAP) for intersecting labels is computed as</span>
<span class="sd">    the average of the average precision (AP) scores for all queries. The average</span>
<span class="sd">    precision for a single query is calculated using the :func:`average_precision_intersecting`</span>
<span class="sd">    function, which considers the intersecting true and predicted labels for the</span>
<span class="sd">    top-k retrieved items.</span>

<span class="sd">    MAP is given by:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{MAP} = \frac{1}{Q} \sum_{q=1}^{Q} \text{AP}_{\text{intersecting}}(q, c, k)</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`Q` is the total number of queries,</span>
<span class="sd">    - :math:`\text{AP}_{\text{intersecting}}(q, c, k)` is the average precision for the</span>
<span class="sd">    :math:`q`-th query, calculated using the intersecting true labels (`q`),</span>
<span class="sd">    predicted labels (`c`), and the number of top items (`k`) to consider.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ap_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">_average_precision_intersecting</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)]</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ap_list</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">ap_list</span><span class="p">)</span></div>



<div class="viewcode-block" id="retrieval_map_macro">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_map_macro">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_map_macro</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the mean average precision at position k for the intersecting labels.</span>

<span class="sd">    This function internally uses :func:`retrieval_map` to calculate the MAP for each query and then</span>
<span class="sd">    applies :func:`macrofy` to perform macro-averaging across multiple queries.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_macrofy</span><span class="p">(</span><span class="n">retrieval_map</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_retrieval_map_numpy</span><span class="p">(</span><span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate mean average precision at position k.</span>

<span class="sd">    The mean average precision (MAP) at position :math:`k` is calculated as follows:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{AP}_q = \frac{1}{|R_q|} \sum_{i=1}^{k} P_q(i) \cdot \mathbb{1}(y_{\text{true},q} = y_{\text{pred},i})</span>

<span class="sd">        \text{MAP}@k = \frac{1}{|Q|} \sum_{q=1}^{Q} \text{AP}_q</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`\text{AP}_q` is the average precision for query :math:`q`,</span>
<span class="sd">    - :math:`P_q(i)` is the precision at the :math:`i`-th position for query :math:`q`,</span>
<span class="sd">    - :math:`\mathbb{1}(y_{\text{true},q} = y_{\text{pred},i})` is the indicator function that equals</span>
<span class="sd">    1 if the true label of the query matches the predicted label at position :math:`i` and 0 otherwise,</span>
<span class="sd">    - :math:`|R_q|` is the total number of relevant items for query :math:`q`,</span>
<span class="sd">    - :math:`|Q|` is the total number of queries.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">relevance_mask</span> <span class="o">=</span> <span class="n">candidates_labels_</span> <span class="o">==</span> <span class="n">query_label_</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">cumulative_relevant</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">relevance_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">precision_at_k</span> <span class="o">=</span> <span class="n">cumulative_relevant</span> <span class="o">*</span> <span class="n">relevance_mask</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">sum_precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">precision_at_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">num_relevant</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">relevance_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">average_precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span>
        <span class="n">sum_precision</span><span class="p">,</span>
        <span class="n">num_relevant</span><span class="p">,</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sum_precision</span><span class="p">),</span>
        <span class="n">where</span><span class="o">=</span><span class="n">num_relevant</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">average_precision</span><span class="p">)</span>  <span class="c1"># type: ignore[no-any-return]</span>


<div class="viewcode-block" id="retrieval_hit_rate">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_hit_rate">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_hit_rate</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the hit rate at position k.</span>

<span class="sd">    The hit rate is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{Hit Rate} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{query},i} \in y_{\text{candidates},i}^{(1:k)})}{N}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`N` is the total number of queries,</span>
<span class="sd">    - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,</span>
<span class="sd">    - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query,</span>
<span class="sd">    - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition</span>
<span class="sd">    is true and 0 otherwise.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>

    <span class="n">num_queries</span> <span class="o">=</span> <span class="n">query_label_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">hit_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">query_label</span> <span class="ow">in</span> <span class="n">candidate_labels</span><span class="p">:</span>
            <span class="n">hit_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">hit_count</span> <span class="o">/</span> <span class="n">num_queries</span>  <span class="c1"># type: ignore[no-any-return]</span></div>



<div class="viewcode-block" id="retrieval_hit_rate_intersecting">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_hit_rate_intersecting">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_hit_rate_intersecting</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the hit rate at position k for the intersecting labels.</span>

<span class="sd">    The intersecting hit rate is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{Hit Rate}_{\text{intersecting}} = \frac{\sum_{i=1}^N \mathbb{1} \left( \sum_{j=1}^k</span>
<span class="sd">        \left( y_{\text{query},i} \cdot y_{\text{candidates},i,j} \right) &gt; 0 \right)}{N}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`N` is the total number of queries,</span>
<span class="sd">    - :math:`y_{\text{query},i}` is the one-hot encoded label vector for the :math:`i`-th query,</span>
<span class="sd">    - :math:`y_{\text{candidates},i,j}` is the one-hot encoded label vector of the :math:`j`-th</span>
<span class="sd">    candidate for the :math:`i`-th query,</span>
<span class="sd">    - :math:`k` is the number of top candidates considered,</span>
<span class="sd">    - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition</span>
<span class="sd">    is true and 0 otherwise.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>

    <span class="n">num_queries</span> <span class="o">=</span> <span class="n">query_label_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">hit_count</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">candidate_labels_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">candidate_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">query_label</span> <span class="o">*</span> <span class="n">candidate_labels_sum</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">hit_count</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">hit_count</span> <span class="o">/</span> <span class="n">num_queries</span>  <span class="c1"># type: ignore[no-any-return]</span></div>



<div class="viewcode-block" id="retrieval_hit_rate_macro">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_hit_rate_macro">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_hit_rate_macro</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the hit rate at position k for the intersecting labels.</span>

<span class="sd">    This function internally uses :func:`retrieval_hit_rate` to calculate the hit rate at position :math:`k`</span>
<span class="sd">    for each query and applies :func:`macrofy` to perform macro-averaging across multiple queries.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_macrofy</span><span class="p">(</span><span class="n">retrieval_hit_rate</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_retrieval_hit_rate_numpy</span><span class="p">(</span><span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the hit rate at position k.</span>

<span class="sd">    The hit rate is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{Hit Rate} = \frac{\sum_{i=1}^N \mathbb{1}(y_{\text{query},i} \in y_{\text{candidates},i}^{(1:k)})}{N}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`N` is the total number of queries,</span>
<span class="sd">    - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,</span>
<span class="sd">    - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query,</span>
<span class="sd">    - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the condition</span>
<span class="sd">    is true and 0 otherwise.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">truncated_candidates</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">hit_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="n">query_label_</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">truncated_candidates</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hit_mask</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># type: ignore[no-any-return]</span>


<div class="viewcode-block" id="retrieval_precision">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_precision">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_precision</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the precision at position k.</span>

<span class="sd">    Precision at position :math:`k` is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{Precision@k} = \frac{1}{N} \sum_{i=1}^N \frac{|y_{\text{query},i} \cap</span>
<span class="sd">        y_{\text{candidates},i}^{(1:k)}|}{k}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`N` is the total number of queries,</span>
<span class="sd">    - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,</span>
<span class="sd">    - :math:`y_{\text{candidates},i}^{(1:k)}` is the set of top-k predicted labels for the :math:`i`-th query.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>

    <span class="n">total_precision</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">num_queries</span> <span class="o">=</span> <span class="n">query_label_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">relevant_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">label</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">candidate_labels</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="n">query_label</span><span class="p">]</span>
        <span class="n">precision_at_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">relevant_items</span><span class="p">)</span> <span class="o">/</span> <span class="n">candidate_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="n">total_precision</span> <span class="o">+=</span> <span class="n">precision_at_k</span>

    <span class="k">return</span> <span class="n">total_precision</span> <span class="o">/</span> <span class="n">num_queries</span>  <span class="c1"># type: ignore[no-any-return]</span></div>



<div class="viewcode-block" id="retrieval_precision_intersecting">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_precision_intersecting">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_precision_intersecting</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the precision at position k for the intersecting labels.</span>

<span class="sd">    Precision at position :math:`k` for intersecting labels is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{Precision@k}_{\text{intersecting}} = \frac{1}{N} \sum_{i=1}^N</span>
<span class="sd">        \frac{\sum_{j=1}^k \mathbb{1} \left( y_{\text{query},i} \cdot y_{\text{candidates},i,j} &gt; 0 \right)}{k}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`N` is the total number of queries,</span>
<span class="sd">    - :math:`y_{\text{query},i}` is the one-hot encoded label vector for the :math:`i`-th query,</span>
<span class="sd">    - :math:`y_{\text{candidates},i,j}` is the one-hot encoded label vector of the :math:`j`-th</span>
<span class="sd">    candidate for the :math:`i`-th query,</span>
<span class="sd">    - :math:`k` is the number of top candidates considered,</span>
<span class="sd">    - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the</span>
<span class="sd">    condition is true and 0 otherwise.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>

    <span class="n">total_precision</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">num_queries</span> <span class="o">=</span> <span class="n">query_label_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># (n_classes,), (n_candidates, n_classes)</span>

        <span class="n">relevant_items</span> <span class="o">=</span> <span class="p">[</span><span class="n">label</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">candidate_labels</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">query_label</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">precision_at_k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">relevant_items</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">candidate_labels</span><span class="p">)</span>

        <span class="n">total_precision</span> <span class="o">+=</span> <span class="n">precision_at_k</span>

    <span class="k">return</span> <span class="n">total_precision</span> <span class="o">/</span> <span class="n">num_queries</span>  <span class="c1"># type: ignore[no-any-return]</span></div>



<div class="viewcode-block" id="retrieval_precision_macro">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_precision_macro">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_precision_macro</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the precision at position k for the intersecting labels.</span>

<span class="sd">    This function internally uses :func:`retrieval_precision` to calculate the precision at position :math:`k`</span>
<span class="sd">    for each query and applies :func:`macrofy` to perform macro-averaging across multiple queries.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_macrofy</span><span class="p">(</span><span class="n">retrieval_precision</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_retrieval_precision_numpy</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the precision at position k.</span>

<span class="sd">    Precision at position :math:`k` is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{Precision@k} = \frac{1}{N} \sum_{i=1}^N \frac{\sum_{j=1}^k</span>
<span class="sd">        \mathbb{1}(y_{\text{query},i} = y_{\text{candidates},i,j})}{k}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`N` is the total number of queries,</span>
<span class="sd">    - :math:`y_{\text{query},i}` is the true label for the :math:`i`-th query,</span>
<span class="sd">    - :math:`y_{\text{candidates},i,j}` is the :math:`j`-th predicted label for the :math:`i`-th query,</span>
<span class="sd">    - :math:`\mathbb{1}(\text{condition})` is the indicator function that equals 1 if the</span>
<span class="sd">    condition is true and 0 otherwise,</span>
<span class="sd">    - :math:`k` is the number of top candidates considered.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">top_k_candidates</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">matches</span> <span class="o">=</span> <span class="p">(</span><span class="n">top_k_candidates</span> <span class="o">==</span> <span class="n">query_label_</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">relevant_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">precision_at_k</span> <span class="o">=</span> <span class="n">relevant_counts</span> <span class="o">/</span> <span class="n">k</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precision_at_k</span><span class="p">)</span>  <span class="c1"># type: ignore[no-any-return]</span>


<span class="k">def</span> <span class="nf">_dcg</span><span class="p">(</span><span class="n">relevance_scores</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Discounted Cumulative Gain (DCG) at position k.</span>

<span class="sd">    DCG is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{DCG@k} = \sum_{i=1}^k \frac{r_i}{\log_2(i + 1)}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`r_i` is the relevance score of the item at rank :math:`i`,</span>
<span class="sd">    - :math:`k` is the number of top items considered.</span>

<span class="sd">    :param relevance_scores: numpy array of relevance scores for items</span>
<span class="sd">    :param k: the number of top items to consider</span>
<span class="sd">    :return: DCG value at position k</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relevance_scores</span> <span class="o">=</span> <span class="n">relevance_scores</span><span class="p">[:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">discounts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">relevance_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">relevance_scores</span> <span class="o">/</span> <span class="n">discounts</span><span class="p">)</span>  <span class="c1"># type: ignore[no-any-return]</span>


<span class="k">def</span> <span class="nf">_idcg</span><span class="p">(</span><span class="n">relevance_scores</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Ideal Discounted Cumulative Gain (IDCG) at position k.</span>

<span class="sd">    IDCG is the maximum possible DCG that can be achieved if the relevance</span>
<span class="sd">    scores are sorted in descending order. It is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{IDCG@k} = \sum_{i=1}^k \frac{r_i^{\text{ideal}}}{\log_2(i + 1)}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`r_i^{\text{ideal}}` is the relevance score of the item at rank :math:`i` in the ideal (sorted) order,</span>
<span class="sd">    - :math:`k` is the number of top items considered.</span>

<span class="sd">    :param relevance_scores: `np.array` of relevance scores for items</span>
<span class="sd">    :param k: the number of top items to consider</span>
<span class="sd">    :return: IDCG value at position k</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">ideal_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">relevance_scores</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">_dcg</span><span class="p">(</span><span class="n">ideal_scores</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>


<div class="viewcode-block" id="retrieval_ndcg">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_ndcg">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_ndcg</span><span class="p">(</span><span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k.</span>

<span class="sd">    NDCG at position :math:`k` is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{NDCG@k} = \frac{\text{DCG@k}}{\text{IDCG@k}}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`\text{DCG@k}` is the Discounted Cumulative Gain at position :math:`k`,</span>
<span class="sd">    - :math:`\text{IDCG@k}` is the Ideal Discounted Cumulative Gain at position :math:`k`.</span>

<span class="sd">    The NDCG value is normalized such that it is between 0 and 1, where 1 indicates the ideal ranking.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model</span>
<span class="sd">     (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">query_label_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>

    <span class="n">ndcg_scores</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">relevance_scores</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">query_label_</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">==</span> <span class="n">candidates_labels_</span>

    <span class="k">for</span> <span class="n">rel_scores</span> <span class="ow">in</span> <span class="n">relevance_scores</span><span class="p">:</span>
        <span class="n">cur_dcg</span> <span class="o">=</span> <span class="n">_dcg</span><span class="p">(</span><span class="n">rel_scores</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">cur_idcg</span> <span class="o">=</span> <span class="n">_idcg</span><span class="p">(</span><span class="n">rel_scores</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">ndcg_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span> <span class="k">if</span> <span class="n">cur_idcg</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">cur_dcg</span> <span class="o">/</span> <span class="n">cur_idcg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ndcg_scores</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value]</span></div>



<div class="viewcode-block" id="retrieval_ndcg_intersecting">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_ndcg_intersecting">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_ndcg_intersecting</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k for the intersecting labels.</span>

<span class="sd">    NDCG at position :math:`k` for intersecting labels is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{NDCG@k}_{\text{intersecting}} = \frac{\text{DCG@k}_{\text{intersecting}}}</span>
<span class="sd">        {\text{IDCG@k}_{\text{intersecting}}}</span>

<span class="sd">    where:</span>

<span class="sd">    - :math:`\text{DCG@k}_{\text{intersecting}}` is the Discounted Cumulative Gain for the intersecting labels at position :math:`k`,</span>
<span class="sd">    - :math:`\text{IDCG@k}_{\text{intersecting}}` is the Ideal Discounted Cumulative Gain for the intersecting labels at position :math:`k`.</span>

<span class="sd">    Intersecting relevance is determined by checking whether the query labels overlap with</span>
<span class="sd">    the candidate labels.</span>
<span class="sd">    NDCG values are normalized between 0 and 1, where 1 indicates the ideal ranking.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">query_labels_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">ndcg_scores</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">expanded_relevance_scores</span><span class="p">:</span> <span class="n">npt</span><span class="o">.</span><span class="n">NDArray</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">bool</span><span class="p">]</span> <span class="o">=</span> <span class="n">query_labels_</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">candidates_labels_</span>
    <span class="n">relevance_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">expanded_relevance_scores</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">rel_scores</span> <span class="ow">in</span> <span class="n">relevance_scores</span><span class="p">:</span>
        <span class="n">cur_dcg</span> <span class="o">=</span> <span class="n">_dcg</span><span class="p">(</span><span class="n">rel_scores</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">cur_idcg</span> <span class="o">=</span> <span class="n">_idcg</span><span class="p">(</span><span class="n">rel_scores</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">ndcg_scores</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mf">0.0</span> <span class="k">if</span> <span class="n">cur_idcg</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">cur_dcg</span> <span class="o">/</span> <span class="n">cur_idcg</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ndcg_scores</span><span class="p">)</span>  <span class="c1"># type: ignore[return-value]</span></div>



<div class="viewcode-block" id="retrieval_ndcg_macro">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_ndcg_macro">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_ndcg_macro</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Normalized Discounted Cumulative Gain (NDCG) at position k for the intersecting labels.</span>

<span class="sd">    This function calculates NDCG using :func:`retrieval_ndcg` and applies it to each</span>
<span class="sd">    query using :func:`macrofy` to compute the macro-averaged score.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="k">return</span> <span class="n">_macrofy</span><span class="p">(</span><span class="n">retrieval_ndcg</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>



<div class="viewcode-block" id="retrieval_mrr">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_mrr">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_mrr</span><span class="p">(</span><span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Mean Reciprocal Rank (MRR) at position k.</span>

<span class="sd">    MRR is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{MRR@k} = \frac{1}{N} \sum_{i=1}^N \frac{1}{\text{rank}_i}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`\text{rank}_i` is the rank position of the first relevant item in the top-k results for query :math:`i`,</span>
<span class="sd">    - :math:`N` is the total number of queries.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">query_labels_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>

    <span class="n">mrr_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">num_queries</span> <span class="o">=</span> <span class="n">query_labels_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_labels_</span><span class="p">,</span> <span class="n">candidates_labels_</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_labels</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="n">query_label</span><span class="p">:</span>
                <span class="n">mrr_sum</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">break</span>

    <span class="k">return</span> <span class="n">mrr_sum</span> <span class="o">/</span> <span class="n">num_queries</span>  <span class="c1"># type: ignore[no-any-return]</span></div>



<div class="viewcode-block" id="retrieval_mrr_intersecting">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_mrr_intersecting">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_mrr_intersecting</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Mean Reciprocal Rank (MRR) at position k for the intersecting labels.</span>

<span class="sd">    MRR is calculated as:</span>

<span class="sd">    .. math::</span>

<span class="sd">        \text{MRR@k}_{\text{intersecting}} = \frac{1}{N} \sum_{i=1}^N \frac{1}{\text{rank}_i}</span>

<span class="sd">    where:</span>
<span class="sd">    - :math:`\text{rank}_i` is the rank position of the first relevant (intersecting) item in the top-k</span>
<span class="sd">    results for query :math:`i`,</span>
<span class="sd">    - :math:`N` is the total number of queries.</span>

<span class="sd">    Intersecting relevance is determined by checking whether the query label intersects with the candidate labels.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="n">query_labels_</span><span class="p">,</span> <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">)</span>
    <span class="n">candidates_labels_</span> <span class="o">=</span> <span class="n">candidates_labels_</span><span class="p">[:,</span> <span class="p">:</span><span class="n">k</span><span class="p">]</span>
    <span class="n">mrr_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">num_queries</span> <span class="o">=</span> <span class="n">query_labels_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">query_label</span><span class="p">,</span> <span class="n">candidate_labels</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">query_labels_</span><span class="p">,</span> <span class="n">candidates_labels_</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_labels</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">label</span> <span class="o">*</span> <span class="n">query_label</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mrr_sum</span> <span class="o">+=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">break</span>

    <span class="k">return</span> <span class="n">mrr_sum</span> <span class="o">/</span> <span class="n">num_queries</span>  <span class="c1"># type: ignore[no-any-return]</span></div>



<div class="viewcode-block" id="retrieval_mrr_macro">
<a class="viewcode-back" href="../../../autoapi/autointent/metrics/retrieval/index.html#autointent.metrics.retrieval_mrr_macro">[docs]</a>
<span class="k">def</span> <span class="nf">retrieval_mrr_macro</span><span class="p">(</span>
    <span class="n">query_labels</span><span class="p">:</span> <span class="n">LABELS_VALUE_TYPE</span><span class="p">,</span>
    <span class="n">candidates_labels</span><span class="p">:</span> <span class="n">CANDIDATE_TYPE</span><span class="p">,</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the Mean Reciprocal Rank (MRR) at position k for the intersecting labels.</span>

<span class="sd">    This function calculates MRR using :func:`retrieval_mrr` and applies it to each</span>
<span class="sd">    query using :func:`macrofy` to compute the macro-averaged score.</span>

<span class="sd">    :param query_labels: For each query, this list contains its class labels</span>
<span class="sd">    :param candidates_labels: For each query, these lists contain class labels of items ranked by a retrieval model (from most to least relevant)</span>
<span class="sd">    :param k: Number of top items to consider for each query</span>
<span class="sd">    :return: Score of the retrieval metric</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa: E501</span>
    <span class="k">return</span> <span class="n">_macrofy</span><span class="p">(</span><span class="n">retrieval_mrr</span><span class="p">,</span> <span class="n">query_labels</span><span class="p">,</span> <span class="n">candidates_labels</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span></div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">AutoIntent</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../concepts.html">Key Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guides/index.html">Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../learn/index.html">Learn AutoIntent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autoapi/index.html">API Reference</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2024, DeepPavlov.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.1.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>